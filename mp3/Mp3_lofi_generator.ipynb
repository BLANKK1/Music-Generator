{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9120ac3-a25e-4fea-a372-a9fa352727ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9164447c-182f-4b09-86ea-ab527a9ced37",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp3_dir = 'Music Data/New_mp3/'\n",
    "mel_dir = 'mel_spectrograms'\n",
    "os.makedirs(mel_dir, exist_ok=True)\n",
    "\n",
    "mp3_files = [f for f in os.listdir(mp3_dir) if f.endswith('.mp3')]\n",
    "\n",
    "for mp3_file in mp3_files:\n",
    "    file_path = os.path.join(mp3_dir, mp3_file)\n",
    "    print(f\"Processing {mp3_file}...\")\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=22050)\n",
    "        mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "        mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "\n",
    "        out_file = os.path.splitext(mp3_file)[0] + '.npy'\n",
    "        np.save(os.path.join(mel_dir, out_file), mel_db)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed on {mp3_file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84ce342-dae9-4a81-95df-573110baee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "mel_file = np.load(os.path.join(mel_dir, os.listdir(mel_dir)[0]))\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(mel_file, x_axis='time', y_axis='mel', sr=16000)\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel Spectrogram')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da753a86-4931-4503-87db-412d1cdf6847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "mel_dir = 'mel_spectrograms'\n",
    "mel_data = []\n",
    "\n",
    "for file in os.listdir(mel_dir):\n",
    "    if file.endswith('.npy'):\n",
    "        mel = np.load(os.path.join(mel_dir, file))\n",
    "        # Pad or crop to fixed length (e.g., 128 x 256)\n",
    "        if mel.shape[1] >= 256:\n",
    "            mel = mel[:, :256]\n",
    "        else:\n",
    "            pad_width = 256 - mel.shape[1]\n",
    "            mel = np.pad(mel, ((0,0), (0,pad_width)), mode='constant')\n",
    "\n",
    "        mel_data.append(mel)\n",
    "\n",
    "mel_data = np.array(mel_data)\n",
    "mel_data = (mel_data - mel_data.min()) / (mel_data.max() - mel_data.min())  \n",
    "\n",
    "print(\"Spectrogram data shape:\", mel_data.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af1174d-d357-4ab5-b8b3-8e9064f843b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "mel_tensor = torch.tensor(mel_data, dtype=torch.float32).unsqueeze(1)  # (B, 1, 128, 256)\n",
    "train_loader = DataLoader(TensorDataset(mel_tensor), batch_size=16, shuffle=True)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=64):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1),  # (16, 64, 128)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),  # (32, 32, 64)\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(32*32*64, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(32*32*64, latent_dim)\n",
    "        self.fc_decode = nn.Linear(latent_dim, 32*32*64)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Unflatten(1, (32, 32, 64)),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, output_padding=1, padding=1),  # (16, 64, 128)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, output_padding=1, padding=1),  # (1, 128, 256)\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        h_decoded = self.fc_decode(z)\n",
    "        x_hat = self.decoder(h_decoded)\n",
    "        return x_hat, mu, logvar\n",
    "\n",
    "model = VAE()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Loss function\n",
    "def vae_loss(x_hat, x, mu, logvar):\n",
    "    recon_loss = nn.functional.mse_loss(x_hat, x)\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)\n",
    "    return recon_loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f9355b-be06-41ac-b089-e1618cfaa244",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "model.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        x = batch[0]\n",
    "        x_hat, mu, logvar = model(x)\n",
    "        loss = vae_loss(x_hat, x, mu, logvar)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9a27d8-92af-4a9b-ae27-8ee39a78f5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(1, 64)  # Sample random latent vector\n",
    "    h_decoded = model.fc_decode(z)\n",
    "    gen_mel = model.decoder(h_decoded).squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842e641e-7287-4afa-a2ab-80d72eabed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "# Convert back to waveform using Griffin-Lim\n",
    "gen_mel = gen_mel * (mel_data.max() - mel_data.min()) + mel_data.min()  # De-normalize\n",
    "audio = librosa.feature.inverse.mel_to_audio(librosa.db_to_power(gen_mel), sr=22050)\n",
    "\n",
    "import soundfile as sf\n",
    "sf.write('output_lofi.wav', audio, samplerate=22050)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288414d0-f3e5-4609-8e13-10ee966a6288",
   "metadata": {},
   "source": [
    "Whole Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684396f2-d84c-4417-8773-bd621f69b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment, effects\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 1: Load MP3s and extract mel spectrograms\n",
    "# -------------------------------\n",
    "mp3_dir = 'Music Data/New_mp3/'\n",
    "mel_dir = 'mel_spectrograms'\n",
    "os.makedirs(mel_dir, exist_ok=True)\n",
    "\n",
    "mp3_files = [f for f in os.listdir(mp3_dir) if f.endswith('.mp3')]\n",
    "for mp3_file in mp3_files:\n",
    "    file_path = os.path.join(mp3_dir, mp3_file)\n",
    "    print(f\"Processing {mp3_file}...\")\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=22050)\n",
    "        mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "        mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "\n",
    "        out_file = os.path.splitext(mp3_file)[0] + '.npy'\n",
    "        np.save(os.path.join(mel_dir, out_file), mel_db)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed on {mp3_file}: {e}\")\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 2: Load mel spectrograms and prepare dataset\n",
    "# -------------------------------\n",
    "mel_data = []\n",
    "for file in os.listdir(mel_dir):\n",
    "    if file.endswith('.npy'):\n",
    "        mel = np.load(os.path.join(mel_dir, file))\n",
    "        if mel.shape[1] >= 256:\n",
    "            mel = mel[:, :256]\n",
    "        else:\n",
    "            pad_width = 256 - mel.shape[1]\n",
    "            mel = np.pad(mel, ((0, 0), (0, pad_width)), mode='constant')\n",
    "        mel_data.append(mel)\n",
    "\n",
    "mel_data = np.array(mel_data)\n",
    "mel_data = (mel_data - mel_data.min()) / (mel_data.max() - mel_data.min())  # Normalize to [0,1]\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 3: Define VAE model\n",
    "# -------------------------------\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=64):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(32 * 32 * 64, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(32 * 32 * 64, latent_dim)\n",
    "        self.fc_decode = nn.Linear(latent_dim, 32 * 32 * 64)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Unflatten(1, (32, 32, 64)),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, output_padding=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, output_padding=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        h_decoded = self.fc_decode(z)\n",
    "        x_hat = self.decoder(h_decoded)\n",
    "        return x_hat, mu, logvar\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 4: Train the VAE\n",
    "# -------------------------------\n",
    "mel_tensor = torch.tensor(mel_data, dtype=torch.float32).unsqueeze(1)\n",
    "train_loader = DataLoader(TensorDataset(mel_tensor), batch_size=16, shuffle=True)\n",
    "model = VAE()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def vae_loss(x_hat, x, mu, logvar):\n",
    "    recon_loss = nn.functional.mse_loss(x_hat, x)\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)\n",
    "    return recon_loss + kl_loss\n",
    "\n",
    "EPOCHS = 12\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        x = batch[0]\n",
    "        x_hat, mu, logvar = model(x)\n",
    "        loss = vae_loss(x_hat, x, mu, logvar)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 5: Generate multiple chunks and stitch into 2–3 minute audio\n",
    "# -------------------------------\n",
    "model.eval()\n",
    "generated_chunks = []\n",
    "num_chunks = 30  # Each chunk ≈ 6s → 30 x 6s ≈ 180s = 3 minutes\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_chunks):\n",
    "        z = torch.randn(1, 64)\n",
    "        h_decoded = model.fc_decode(z)\n",
    "        chunk = model.decoder(h_decoded).squeeze().numpy()\n",
    "        generated_chunks.append(chunk)\n",
    "\n",
    "gen_mel = np.concatenate(generated_chunks, axis=1)\n",
    "gen_mel = gen_mel * (mel_data.max() - mel_data.min()) + mel_data.min()  # De-normalize\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 6: Convert Mel to Audio\n",
    "# -------------------------------\n",
    "audio = librosa.feature.inverse.mel_to_audio(librosa.db_to_power(gen_mel), sr=22050)\n",
    "sf.write('output_lofi.wav', audio, samplerate=22050)\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 7: Add Lo-fi effect (Low-pass filter)\n",
    "# -------------------------------\n",
    "sound = AudioSegment.from_wav('output_lofi.wav')\n",
    "sound = effects.low_pass_filter(sound, cutoff=3500)\n",
    "sound.export('output_lofi_lofiStyle.wav', format='wav')\n",
    "\n",
    "print(\"✅ Generated lo-fi audio: output_lofi_lofiStyle.wav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

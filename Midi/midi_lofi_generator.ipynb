{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ffc476-61cf-4f32-ac0d-4b0e7a318342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. INSTALL DEPENDENCIES (run in Colab or Jupyter)\n",
    "# !pip install pretty_midi miditok torch numpy\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import miditok\n",
    "from miditoolkit import MidiFile\n",
    "\n",
    "# 2. TOKENIZE MIDI FILES\n",
    "class MIDIDataset(Dataset):\n",
    "    def __init__(self, midi_folder, tokenizer, max_len=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.midi_paths = []\n",
    "        self.valid_midi_count = 0\n",
    "\n",
    "        all_files = os.listdir(midi_folder)\n",
    "        print(f\"üü° Found {len(all_files)} MIDI files. Checking validity...\")\n",
    "\n",
    "        for file in all_files:\n",
    "            if file.endswith(\".mid\") or file.endswith(\".midi\"):\n",
    "                full_path = os.path.join(midi_folder, file)\n",
    "                try:\n",
    "                    midi = MidiFile(full_path)\n",
    "                    _ = self.tokenizer(midi)\n",
    "                    self.midi_paths.append(full_path)\n",
    "                    self.valid_midi_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Skipping file {file}: {e}\")\n",
    "\n",
    "        print(f\"‚úÖ {self.valid_midi_count} valid MIDI files loaded.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.midi_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.midi_paths[idx]\n",
    "        try:\n",
    "            midi = MidiFile(path)\n",
    "            tokens = self.tokenizer(midi)\n",
    "            # Flatten tokens from all tracks\n",
    "            all_ids = []\n",
    "            for t in tokens:\n",
    "                if hasattr(t, \"ids\"):\n",
    "                    all_ids.extend(t.ids)\n",
    "            \n",
    "            token_ids = torch.tensor(all_ids, dtype=torch.long)\n",
    "\n",
    "            # Pad or truncate\n",
    "            if len(token_ids) < self.max_len:\n",
    "                token_ids = F.pad(token_ids, (0, self.max_len - len(token_ids)))\n",
    "            else:\n",
    "                token_ids = token_ids[:self.max_len]\n",
    "\n",
    "            # Model usually needs input and target (shifted version)\n",
    "            input_ids = token_ids[:-1]\n",
    "            target_ids = token_ids[1:]\n",
    "\n",
    "            return input_ids, target_ids\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error reading {path}: {e}\")\n",
    "            zero = torch.zeros(self.max_len - 1, dtype=torch.long)\n",
    "            return zero, zero\n",
    "\n",
    "# 3. DEFINE TRANSFORMER MODEL\n",
    "class MusicTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = nn.Parameter(torch.randn(1, 10000, d_model))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.decoder = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.pos_encoder[:, :x.size(1)]\n",
    "        x = self.transformer(x)\n",
    "        return self.decoder(x)\n",
    "\n",
    "# 4. TRAINING LOOP\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Training on device:\", device)\n",
    "def train(model, dataloader, vocab_size, device):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(5):  # change to more epochs later\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output = model(x)\n",
    "            loss = loss_fn(output.view(-1, vocab_size), y.view(-1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "# 5. GENERATE MUSIC\n",
    "\n",
    "def generate(model, tokenizer, seed_ids, max_len=10000, device='cuda'):\n",
    "    model.eval()\n",
    "    generated = seed_ids[:]\n",
    "    input_ids = torch.tensor(generated).unsqueeze(0).to(device)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids)\n",
    "            next_token = torch.argmax(logits[0, -1]).item()\n",
    "            generated.append(next_token)\n",
    "            input_ids = torch.tensor(generated[-512:]).unsqueeze(0).to(device)\n",
    "\n",
    "    return tokenizer.decode([generated])\n",
    "\n",
    "# 6. EXECUTE EVERYTHING\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    midi_dir = \"lofi_midis/\" # <- your 1200 midi files here\n",
    "    tokenizer = miditok.REMI()\n",
    "    dataset = MIDIDataset(midi_dir, tokenizer)\n",
    "    print(f\"Total valid files for training: {len(dataset)}\")\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    model = MusicTransformer(vocab_size)\n",
    "\n",
    "    train(model, dataloader, vocab_size,device)\n",
    "\n",
    "    seed = dataset[0][0][:100].tolist()  # take first 100 tokens of one file\n",
    "    new_midi = generate(model, tokenizer, seed)\n",
    "    new_midi.dump(\"generated_lofi.mid\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
